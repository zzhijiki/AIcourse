# README

这是我们学校AI课的代码作业，

课程设置是：上半学期讲李航统计学习方法的监督学习，下半学期讲深度学习CNN,RNN,GAN等

代码主要涉及，python的numpy和pytorch。

这是我的平时作业的课程成绩，其实还是可以的：

HW1  : 10.0 + 2.0 
HW2  : 10.0 + 0.0 
HW3  : 10.0 + 1.0 
HW4  : 10.0 + 0.0 
HW5  : 10.0 + 5.0 
HW6  : 9.0 + 0.0 
HW7  : 15.0 + 0.0 

Total : 75.0/75.0
作业成绩占比 : 45.0/45
Rank : 1/24

|       课程名       |   学年    | 学期 | 学分 | 平时成绩 | 期中成绩 | 期末成绩 | 总成绩 | 是否通过 |
| :----------------: | :-------: | :--: | :--: | :------: | :------: | :------: | :----: | :------: |
| 人工智能的数学基础 | 2019-2020 |  1   |  4   |   100    |   null   |    95    |   98   |   通过   |

**作业1：**

- **梯度下降法** 和 **BFGS**（Armijo条件） 求解函数求函数的极值：无聊的同时顺便做了一个梯度检验。

**作业2：**

- 证明题：关于当损失函数是对数损失函数时，经验风险最小化等价于极大似然估计。

- **感知机**分类器：他竟然要我用感知机分类一个不可分的题目，做了一点优化，但没什么用，感知机太垃圾了。

- 最近邻**KNN**：关于KDTREE实在是太麻烦了，因为我老想着那个排序要用partitial sort，然后就放弃了，其实用普通的sort就可以了，还好KNN用的不多，所以也没必要较真，用写了个naive knn。

**作业3：**

- 熟悉python的class：拟合一个曲线

- **朴素贝叶斯**：for循环比较绕，下标ijk分清楚谁是谁很困难。

- **决策树**：当时还不会写递归，就没做出来，害，太菜了。不过这道题也没算分，因为就没人会做。

- **Logistic回归**：做了三分类的logistic回归（其实是softmax），不知道对不对，我估摸着可能哪里写错了

**作业4：（小测验）**

- 刚上完统计学习方法，Deep Learning课都没上过，叫我们当场推导数，写一个神经网络（用numpy）可还行，也就我一个人做完了，还是压哨写完的。

**作业5：**

- **SVM**：这个算法我可太装逼了，用两个类包装了三个不同的SVM算法（对hinge loss用梯度下降法，用SQP算法解对偶方程的nonlinear kernel函数，SMO），还用了spicy求解器，可能这是我最优化理论的巅峰了。

- **AdaBoost**：用树桩的adaboost做三分类还挺有意思的。
- **EM**算法习题9.3：高斯混合模型，直接套公式就行了
- **隐马尔可夫HMM**模型习题10.1

**作业6：**

- 利用**Pytorch**的自动微分框架来重新实现作业一

- 利用**Pytorch**的`torch.nn.Module`来重新实现小测验
  - 网络层自己用`torch.nn.Module`实现
  - 优化器和loss使用现成的即可

- 借助**Pytorch**文档(custom_optimizer)的代码结构，实现带动量的随机梯度下降算法(SGD with momentum)

**作业7：**

- 大作业：图像分类任务**Cinic-10**，比较新的小的基准数据集 

  用了：resnet的迁移学习，图像增强，label-smoothing，warmup的lr_scheduler，finetune

  结果：测试集正确率达到了91%-92%
  
  参考 https://www.flyai.com/d/Cinic-10_V1 我的成绩还是挺不错的，还能排到练习赛的前10。
  
  用过小型的densenet，技术不太行，train不到80。
