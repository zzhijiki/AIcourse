{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 支持向量机 SVM\n",
    "\n",
    "- 利用SVM算法来对iris数据集进行分类处理：\n",
    "- 实现了打星号（**）的算法 1和4和6\n",
    " \n",
    "> 1.利用算法1和梯度下降法求解7.63**** \n",
    "- primal model: 算法1\n",
    "- primal model with hinge loss: (7.63)\n",
    "- 解释：hingeloss没有约束条件可以直接梯度下降求得w和b\n",
    "\n",
    "> 2.利用算法1和梯度下降法求解7.32-7.34\n",
    "- primal model: 算法1\n",
    "- primal model: (7.32-7.34)\n",
    "- 解释：primalmodel具有约束条件，要使用带约束问题的梯度下降法求得w和b\n",
    "\n",
    "> 3.利用算法2/3和梯度下降法求解7.37-7.39\n",
    "- dual model: 算法2、3\n",
    "- dual model: (7.37-7.39)\n",
    "- 解释：dualmodel具有约束条件，要使用带约束问题的梯度下降法求得alpha1234…，进而求得w和b，alpha的梯度看上去更容易求得\n",
    "\n",
    "> 4.利用算法2/3和梯度下降法求解7.95-7.97 – 1 bonus****\n",
    "- dual model: 算法2、3\n",
    "- 非线性SVM模型（7.95-7.97）\n",
    "- dual model with non-liniear kernel: (7.88), (7.90)\n",
    "- 解释：与3没有什么不同，多了一个非线性的核函数\n",
    "\n",
    "> 5.利用SMO算法求解7.37-7.39 – 2 bonus\n",
    "- SMO:算法\n",
    "- dual model: (7.37-7.39)\n",
    "- 解释：dualmodel具有约束条件，使用smo算法，用启发式的方法求得两个alpha的最值。\n",
    "\n",
    "> 6.利用SMO算法求解7.95-7.97 – 3 bonus****\n",
    "- SMO:算法\n",
    "- 非线性SVM模型（7.95-7.97）\n",
    "- dual model with non-linear kernel: (7.88), (7.90)\n",
    "- 解释：与5没什么不同，多了核函数。 \n",
    "\n",
    "> 利用梯度下降法求解有约束问题一般来说也有几种方式，例如：\n",
    "- 惩罚函数法：将约束问题转换为无约束问题，然后再利用梯度下降法求解\n",
    "- 梯度投影法：将梯度下降后的结果再投影到可行域中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T18:37:28.017579Z",
     "start_time": "2019-11-29T18:37:27.091526Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "from numpy import *\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T18:37:28.025579Z",
     "start_time": "2019-11-29T18:37:28.019579Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_iris(ratio=0.8):\n",
    "    features, target = datasets.load_iris(True)\n",
    "    \n",
    "    # 只保留0,1分类 -- 把任务变成二分类任务\n",
    "    idx = np.bitwise_or(target == 0, target == 1)\n",
    "    features = features[idx]\n",
    "    target = target[idx]\n",
    "    \n",
    "    num_samples = len(target)\n",
    "    num_train = math.ceil(num_samples * ratio)\n",
    "    \n",
    "    \n",
    "    # 随机打乱数据\n",
    "#     np.random.seed(4)\n",
    "    idx = np.random.permutation(np.arange(num_samples))\n",
    "    traindata = features[idx[:num_train]], target[idx[:num_train]]\n",
    "    validdata = features[idx[num_train:]], target[idx[num_train:]]\n",
    "    \n",
    "    return traindata, validdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据预处理：\n",
    "svm 是+1 和-1 分类的，做一些修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T18:37:28.189588Z",
     "start_time": "2019-11-29T18:37:28.027579Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def process_X():\n",
    "    (X_train, Y_train), (X_valid, Y_valid) = load_iris()\n",
    "    for i,value in enumerate(Y_train):\n",
    "        Y_train[i] = -1 if Y_train[i]==0 else 1 \n",
    "    for i,value in enumerate(Y_valid):\n",
    "        Y_valid[i] = -1 if Y_valid[i]==0 else 1 \n",
    "    Y_train=Y_train.reshape(-1,1)\n",
    "    Y_valid=Y_valid.reshape(-1,1)\n",
    "    return (X_train, Y_train), (X_valid, Y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T15:15:52.410208Z",
     "start_time": "2019-11-13T15:15:52.406208Z"
    }
   },
   "source": [
    "#### 代码逻辑\n",
    "##### SVM类：相当于main函数\n",
    "```python\n",
    "def fit(self,***,math_program,algo,***)\n",
    "    if math_program == \"hinge_loss\": # 如果用的是hinge_loss的话\n",
    "        do sth with Math_program类\n",
    "        if algo == \"hinge_loss_algo\": # 如果算法是hinge_loss 的梯度下降法（事实上是adagrad）的话\n",
    "            do sth with Algorithm类\n",
    "    elif math_program ==……:  # 如果用的是其他数学规划类的话\n",
    "        do sth\n",
    "        if algo==……:\n",
    "            do sth\n",
    "    else:\n",
    "        raise NameError(\"your algo is,{}\".format(math_program))\n",
    "    \n",
    "\n",
    "def predict(self,****,isnoliner)：\n",
    "    if not isnoliner: # 如果是线性的话\n",
    "        predict sth\n",
    "    else: # 如果不是线性的话\n",
    "        predict sth\n",
    "\n",
    "def predict_smo():\n",
    "    #前面那个函数写烂了，重写了一个去predictSMO。\n",
    "```\n",
    "\n",
    "##### Math_programming（是SVM的子类）：处理有关数学规划方程（loss表示式）的问题\n",
    "```python\n",
    "def __inti__:\n",
    "    xxxx\n",
    "    \n",
    "def hinge_loss(self, X, Y, w, b):\n",
    "    \"\"\"return 正则化的合页损失函数，一个掩码矩阵(就是矩阵中的哪些行是要求梯度的，即合页损失中大于0的)\"\"\"\n",
    "    return loss,score_matrix\n",
    "\n",
    "def dual_model_nonliner_loss(self, X, Y, alpha):\n",
    "    \"\"\"return Q矩阵：Q_ij=y_i*_y_j*(x_i)*(x_j)\"\"\"\n",
    "    return Q_matrix\n",
    "\n",
    "```\n",
    "\n",
    "##### Algorithm（是Math_programming的子类）：处理有关优化update参数的问题\n",
    "```python\n",
    "def __init__(self, X, Y, math_program, lamda=1,margin_C=1):\n",
    "    super(Algorithm, self).__init__(X, Y, math_program, lamda=lamda,margin_C=margin_C) ## 继承父类\n",
    "\n",
    "def hinge_loss_algo(self, score_matrix, w, b, X, Y, iternum, math_program_name,verbose=True):\n",
    "    \"\"\"\n",
    "    梯度下降:这里用的是adagrad\n",
    "    \"\"\"\n",
    "    \n",
    "def dual_optm_algo(self, Q_matrix,X, Y,math_program_name,alpha,C,verbose=True):\n",
    "    \"\"\"\n",
    "    利用scipy库的优化算法，直接求解带约束的问题中的alpha，再计算b，最后利用非线性\n",
    "    的决策函数求得y^hat。\n",
    "\n",
    "    \"\"\"  \n",
    "def smo():\n",
    "    \"\"\" \n",
    "    好长，把一些函数直接塞进了函数，非常乱\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T18:37:28.388600Z",
     "start_time": "2019-11-29T18:37:28.191589Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self,X_train,Y_train,math_program,algo,iternum=1000,lamda=1,margin_C=1,toler=1):\n",
    "        if math_program == \"hinge_loss\":\n",
    "            method1 = Math_programming(X_train,\n",
    "                                       Y_train,\n",
    "                                       math_program,\n",
    "                                       lamda=lamda)\n",
    "            '''因为是在Math_programming里面初始化的，所以要copy一下放到SVM类里面'''\n",
    "            self.n = method1.n\n",
    "            self.dim = method1.dim\n",
    "            self.w = method1.w\n",
    "            self.b = method1.b\n",
    "            self.lamda = method1.lamda\n",
    "            self.loss = method1.loss\n",
    "\n",
    "            self.loss, score_matrix = method1.hinge_loss(\n",
    "                X_train, Y_train, self.w, self.b)\n",
    "\n",
    "            if algo == \"hinge_loss_algo\":\n",
    "                algo1 = Algorithm(X_train, Y_train, math_program, lamda=lamda)\n",
    "\n",
    "                self.w, self.b, self.lossbox = algo1.hinge_loss_algo(\n",
    "                                                    score_matrix,\n",
    "                                                    self.w,\n",
    "                                                    self.b,\n",
    "                                                    X_train,\n",
    "                                                    Y_train,\n",
    "                                                    iternum=iternum,\n",
    "                                                    math_program_name=method1)\n",
    "\n",
    "            else:\n",
    "                raise NameError(\"your algo is,{}\".format(algo))\n",
    "                \n",
    "        elif math_program == \"dual_model_nonliner_loss\":\n",
    "            method2 = Math_programming(X_train,\n",
    "                                   Y_train,\n",
    "                                   math_program,\n",
    "                                   margin_C=margin_C)\n",
    "            \n",
    "            self.n = method2.n\n",
    "            self.dim = method2.dim\n",
    "            self.alpha = method2.alpha\n",
    "            self.w = method2.w\n",
    "            self.b = method2.b\n",
    "            self.loss = method2.loss\n",
    "            self.C = method2.C\n",
    "            self.X_train = X_train\n",
    "            self.Y_train = Y_train\n",
    "                      \n",
    "            self.Q_matrix = method2.dual_model_nonliner_loss(X_train, Y_train, self.alpha)\n",
    "\n",
    "            if algo == \"dual_optm_algo\":\n",
    "\n",
    "                algo2 = Algorithm(X_train,\n",
    "                                  Y_train,\n",
    "                                  math_program,\n",
    "                                  margin_C=self.C)      \n",
    "                self.alpha, self.b = algo2.dual_optm_algo(self.Q_matrix,\n",
    "                                                      X_train,\n",
    "                                                      Y_train,\n",
    "                                                      method2,\n",
    "                                                      self.alpha,\n",
    "                                                      self.C,\n",
    "                                                      verbose=True)\n",
    "        elif math_program == \"dual_model_smo\":\n",
    "            method3 = Math_programming(X_train, Y_train,math_program,margin_C, toler, iternum, kernel_param=1)\n",
    "            \n",
    "            self.X = method3.X\n",
    "            self.y = method3.y\n",
    "            self.C = method3.C\n",
    "            self.toler = method3.toler\n",
    "            self.kernel_param = method3.kernel_param\n",
    "            self.m = method3.m\n",
    "            self.mapped_data = method3.mapped_data\n",
    "            self.E = method3.E\n",
    "            self.alphas = method3.alphas\n",
    "            self.b = method3.b\n",
    "            \n",
    "            if algo==\"smo_algo\":\n",
    "                algo3 = Algorithm(X_train,Y_train,math_program)   \n",
    "                self.alpha, self.b = algo3.smo(X_train, Y_train,method3 ,margin_C, toler,iternum, self.kernel_param)\n",
    "\n",
    "            else:\n",
    "                raise NameError(\"your algo is,{}\".format(algo))\n",
    "         \n",
    "        else:\n",
    "            raise NameError(\"your math_program is,{}\".format(math_program))\n",
    "            \n",
    "    def predict(self, X_test, isnolinear=False):\n",
    "        numtest = X_test.shape[0]\n",
    "        Y_pred = []\n",
    "        if not isnolinear:\n",
    "            Y_pred = np.sign(X_test.dot(self.w) + self.b)\n",
    "        else:\n",
    "            ### 利用alpha 写出非线性的决策函数\n",
    "            def gaussiantemp(x1, x2):\n",
    "                gamma = 1.0\n",
    "                return np.exp(-gamma * np.linalg.norm(x1 - x2))\n",
    "\n",
    "            for i in range(numtest):\n",
    "                kernel_maxtrix = np.zeros((self.n, 1))\n",
    "                for j in range(self.n):\n",
    "                    kernel_maxtrix[j, :] = gaussiantemp(\n",
    "                        X_test[i, :], self.X_train[j, :])\n",
    "                temp = np.multiply(self.alpha.reshape(self.n, 1), self.Y_train)\n",
    "                temp = np.multiply(temp, kernel_maxtrix)\n",
    "                temp = np.sum(temp)\n",
    "                Y_pred_pred = np.sign(temp + self.b)\n",
    "                Y_pred.append(Y_pred_pred)\n",
    "                \n",
    "            Y_pred = np.array(Y_pred).reshape(-1, 1)\n",
    "        return Y_pred\n",
    "    \n",
    "    def predict_smo(self,X_test, *args):\n",
    "        \"\"\"预测smo的算法\"\"\"\n",
    "        \n",
    "        def gaussker(X,Y,var = 1):\n",
    "            \"\"\"高斯核\"\"\"\n",
    "            a = X.reshape(-1,1,4)\n",
    "            b = Y.reshape(1,-1,4)#reshape一下方便广播\n",
    "            c = a-b   #广播机制\n",
    "            d = np.linalg.norm(c,axis=2)  #对第3维求范数\n",
    "            res = np.exp(-d/(2*var))  #对矩阵每一位求exp\n",
    "            return res\n",
    "\n",
    "            ### 利用alpha 写出非线性的决策函数\n",
    "        ay = np.multiply(self.y,self.alphas)\n",
    "        res = np.dot(gaussker(X_test, self.X),ay) + self.b\n",
    "        res[res>0] = 1\n",
    "        res[res<=0] = -1\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T18:37:28.602612Z",
     "start_time": "2019-11-29T18:37:28.390600Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "class Math_programming(SVM):  ## 数学规划的方程类\n",
    "    def __init__(self, X, Y, math_program, lamda=1, margin_C=1,toler=1, kernel_param=1.0,M=1):\n",
    "        if math_program == \"hinge_loss\":\n",
    "            self.n = X.shape[0]  #80\n",
    "            self.dim = X.shape[1]  #4\n",
    "            self.w = np.random.rand(self.dim, 1)\n",
    "            self.b = 0\n",
    "            self.lamda = lamda\n",
    "            self.loss = 0.0\n",
    "\n",
    "        elif math_program == \"dual_model_nonliner_loss\":\n",
    "            self.n = X.shape[0]  #80\n",
    "            self.dim = X.shape[1]  #4\n",
    "            self.alpha = np.random.rand(self.n, 1)\n",
    "            self.loss = 0.0\n",
    "            self.C = margin_C\n",
    "            self.w = np.random.rand(self.dim, 1)\n",
    "            self.b = 0\n",
    "            self.M=M\n",
    "            \n",
    "        elif math_program == \"dual_model_smo\":\n",
    "            self.X = X\n",
    "            y=Y\n",
    "            self.y = y\n",
    "            C=margin_C\n",
    "            self.C = C\n",
    "            self.toler = toler\n",
    "            self.kernel_param = kernel_param\n",
    "            self.m = shape(X)[0]\n",
    "            self.mapped_data = mat(zeros((self.m, self.m)))\n",
    "\n",
    "            for i in range(self.m):\n",
    "                self.mapped_data[:, i] = self.gaussian_kernel(self.X, X[i, :], self.kernel_param)\n",
    "            self.E = mat(zeros((self.m, 2)))\n",
    "            self.alphas = mat(random.rand(self.m, 1))\n",
    "            self.b = 0\n",
    "    \n",
    "\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def hinge_loss(self, X, Y, w, b):\n",
    "        \"\"\"正则化的合页损失函数，p150 \"\"\"\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "\n",
    "        self.loss = 0.0\n",
    "        score_matrix = 1 - np.multiply(Y, X.dot(self.w) + self.b)\n",
    "        score_matrix[score_matrix <= 0] = 0.0\n",
    "\n",
    "        assert score_matrix.shape == (self.n, 1)\n",
    "\n",
    "        reg = np.squeeze(self.lamda * (self.w.T.dot(self.w)))\n",
    "        self.loss = np.squeeze(np.sum(score_matrix, axis=0)) + reg\n",
    "        return self.loss, score_matrix  ### score_matrix 是正数和0组成的矩阵，n by 1  ### loss是个数\n",
    "\n",
    "    def dual_model_nonliner_loss(self, X, Y, alpha):\n",
    "        \"\"\"\n",
    "        return :\n",
    "            Q_matrix: 一个矩阵 Q_ij= y_i * y_j * K(xi,xj)\n",
    "            loss: 这里的loss没有考虑约束\n",
    "        \"\"\"\n",
    "        Q_matrix = np.zeros((self.n, self.n))\n",
    "        for i in range(self.n):\n",
    "            for j in range(self.n):\n",
    "                Q_matrix[i, j] = Y[i, :] * Y[j, :] * self.gaussian(\n",
    "                    X[i, :], X[j, :])\n",
    "        \n",
    "        return Q_matrix\n",
    "    \n",
    "\n",
    "    \"\"\"下面是一些辅助的函数\"\"\"\n",
    "    def gaussian(self, x1, x2):\n",
    "        \"\"\"高斯核1\"\"\"\n",
    "        gamma = 1.0\n",
    "        return np.exp(-gamma * np.linalg.norm(x1 - x2))\n",
    "    \n",
    "    def gaussker(self,X,Y,var = 1):\n",
    "        \"\"\"高斯核2\"\"\"\n",
    "        a = X.reshape(-1,1,4)\n",
    "        b = Y.reshape(1,-1,4)\n",
    "        c = a-b  \n",
    "        d = np.linalg.norm(c,axis=2)  \n",
    "        res = np.exp(-d/(2*var))  \n",
    "        return res\n",
    "    \n",
    "    def gaussian_kernel(self,X, l, kernel_param): \n",
    "        \"\"\"高斯核3\"\"\"\n",
    "        sigma = kernel_param \n",
    "        m = shape(X)[0]\n",
    "        mapped_data = mat(zeros((m, 1)))\n",
    "        for i in range(m):\n",
    "            mapped_data[i] = exp(-sum((X[i, :] - l).T * (X[i, :] - l) / (2 * sigma ** 2)))\n",
    "        return mapped_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T18:37:28.786623Z",
     "start_time": "2019-11-29T18:37:28.603612Z"
    },
    "code_folding": [
     1
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "class Algorithm(Math_programming):  ## updata参数的算法类\n",
    "    def __init__(self, X, Y, math_program, lamda=1,margin_C=1):\n",
    "        super(Algorithm, self).__init__(X, Y, math_program, lamda=lamda,margin_C=margin_C)\n",
    "        pass\n",
    "\n",
    "    def hinge_loss_algo(self, score_matrix, w, b, X, Y, iternum, math_program_name,verbose=True):\n",
    "        \"\"\"\n",
    "        梯度下降:adagrad\n",
    "        \"\"\"\n",
    "        n=X.shape[0]\n",
    "        lossbox=[]\n",
    "        minloss=float(\"inf\")\n",
    "        eta=1\n",
    "        lr_w=0.0\n",
    "        lr_b=0.0\n",
    "        for i in range(iternum):\n",
    "            dW = np.zeros((self.dim, 1))\n",
    "            db = 0\n",
    "\n",
    "            self.loss, score_matrix = math_program_name.hinge_loss(X, Y, w, b)\n",
    "            \n",
    "            score_matrix[score_matrix > 0] = 1.0  # N by 1\n",
    "\n",
    "            matrix1 = np.multiply(score_matrix, np.multiply(X, Y))\n",
    "            matrix2 = np.multiply(score_matrix, Y)\n",
    "\n",
    "            dW += (-np.sum(matrix1, axis=0).reshape(self.dim, 1) + 2 * self.lamda * w)     # 4 by 1\n",
    "            db += -np.sum(matrix2, axis=0)  \n",
    "            \n",
    "            lr_w+=dW**2\n",
    "            lr_b+=db**2\n",
    "            w = w - dW*eta/np.sqrt(lr_w)\n",
    "            b = b - db*eta/np.sqrt(lr_b)\n",
    "            \n",
    "            printnum=100\n",
    "            if (i+1)%printnum==0:\n",
    "                #print(\"iternum {} loss:\".format(i), self.loss)\n",
    "                lossbox.append( self.loss )\n",
    "                if self.loss<minloss:\n",
    "                    tempw=w\n",
    "                    tempb=b\n",
    "                    minloss=self.loss\n",
    "                    min_i=i\n",
    "            \n",
    "        w=tempw\n",
    "        b=tempb\n",
    "        print(\"此次adagrad的最小loss为第{}次：{}，预测时使用了这时的w,b\".format(min_i+1,minloss))\n",
    "        if verbose:\n",
    "            bp_x = np.linspace(0, len(lossbox)*printnum, num=len(lossbox), endpoint=True)\n",
    "            bp_y = lossbox\n",
    "            plt.plot(bp_x, bp_y, linewidth=1, linestyle=\"-\",\n",
    "                     color=\"blue\", label=r\"Legend label $\\sin(x)$\")\n",
    "            plt.xlabel(r\"iternum\")\n",
    "            plt.ylabel(r\"loss\")\n",
    "            plt.title(r\"hinge_loss_algo_loss_plot\")\n",
    "            plt.xlim(0, len(lossbox)*printnum)\n",
    "            plt.ylim(min(lossbox)-1, max(lossbox)+1)\n",
    "            plt.show()\n",
    "            \n",
    "        return w, b,lossbox\n",
    "    \n",
    "    \n",
    "    def dual_optm_algo(self, Q_matrix,X, Y,math_program_name,alpha,C,verbose=True):\n",
    "        \"\"\"\n",
    "        利用scipy库的优化算法直接求解带约束的问题中的alpha，再计算b，最后利用非线性\n",
    "        的决策函数求得y^hat。\n",
    "        \n",
    "        \"\"\"\n",
    "        print(\"....正在使用优化算法SQP直接求解alpha...\")\n",
    "        print(\"------------------------------------\")\n",
    "        \n",
    "        def func(x):\n",
    "            \"\"\" 定义目标函数F(x) \"\"\"\n",
    "            return 0.5 * x.T.dot(Q_matrix).dot(x)-np.ones((self.n,1)).T.dot(x)\n",
    "        def eqfun(x):\n",
    "            return Y.T.dot(x)\n",
    "\n",
    "        self.loss= func(alpha)  \n",
    "        x0=alpha # 迭代初始点\n",
    "        #约束条件\n",
    "        cons = ({'type': 'eq', 'fun': eqfun} ,  \n",
    "                {'type': 'ineq', 'fun': lambda x: x-1e-17  },  \n",
    "               { 'type': 'ineq',  'fun': lambda x: C-x-1e-17 })  \n",
    "           \n",
    "        res = minimize(func, x0, method='SLSQP', constraints=cons)\n",
    "        temp=np.clip(res.x,1e-10,C)\n",
    "        temp[temp==1e-10]=0\n",
    "        res = minimize(func, temp, method='SLSQP', constraints=cons)\n",
    "\n",
    "        ## 求出b\n",
    "        for i in range(len(res.x)):\n",
    "            if res.x[i]>0:\n",
    "                temp=i\n",
    "                b=np.squeeze(Y[i,:]).astype(float)\n",
    "                break\n",
    "        for i in range(len(res.x)):           \n",
    "            b -= res.x[i]*np.squeeze(Y[i,:])*math_program_name.gaussian(X[i,:],X[temp,:])\n",
    "            \n",
    "        \n",
    "        if verbose:\n",
    "            print('迭代终止是否成功：', res.success)\n",
    "            print(\"fmin：\",res.fun)\n",
    "            print(\"向量alpha:\",res.x)\n",
    "            print(\"等式约束1误差：\",Y.T.dot(res.x))\n",
    "            print(\"不等式约束1满足个数(alpha=0)：\",np.sum((res.x)==0))\n",
    "            print(\"不等式约束2满足个数(alpha=C)：\",np.sum(abs(C-res.x)<1e-10))\n",
    "           \n",
    "        return res.x,b\n",
    "    \n",
    "    def smo(self,X, y,model_name, C, toler, iter_num, kernel_param):\n",
    "        model = model_name\n",
    "        changed_alphas = 0\n",
    "        current_iter = 0\n",
    "        for i in range(model.m):\n",
    "            changed_alphas += self.iterate(i, model)\n",
    "        print(\"迭代第%d次：changed_alphas个数为 %d\"%(current_iter, changed_alphas))\n",
    "        current_iter += 1\n",
    "        while current_iter < iter_num and changed_alphas > 0:\n",
    "            changed_alphas = 0\n",
    "            # 处理支持向量\n",
    "            alphas_indice = nonzero((model.alphas.A > 0) * (model.alphas.A < C))[0]\n",
    "            for i in alphas_indice:\n",
    "                changed_alphas += self.iterate(i, model)\n",
    "            print(\"迭代第%d次：changed_alphas个数为 %d\"%(current_iter, changed_alphas))\n",
    "            current_iter += 1\n",
    "        print(\"退出迭代\")\n",
    "        return model.alphas, model.b\n",
    "    \n",
    "    \"\"\"smo的辅助函数iterate\"\"\"\n",
    "    def iterate(self,i, model):\n",
    "        \n",
    "        def clip_alpha(L, H, alpha):\n",
    "            if alpha > H:\n",
    "                alpha = H\n",
    "            elif alpha < L:\n",
    "                alpha = L\n",
    "            return alpha\n",
    "\n",
    "        def calc_b(b1, b2):\n",
    "            return (b1 + b2) / 2\n",
    "\n",
    "        def calc_E(i, model):\n",
    "            yi = float(model.y[i])\n",
    "            gxi = float(multiply(model.alphas, model.y).T * model.mapped_data[:, i] + model.b)\n",
    "            Ei = gxi - yi\n",
    "            return Ei\n",
    "\n",
    "        def select_j(Ei, i, model):\n",
    "            nonzero_indices = nonzero(model.E[:, 0].A)[0]\n",
    "            Ej = 0\n",
    "            j = 0\n",
    "            max_delta = 0\n",
    "            if len(nonzero_indices) > 1:\n",
    "                for index in nonzero_indices:\n",
    "                    if index == i:\n",
    "                        continue\n",
    "                    E_temp = calc_E(index, model)\n",
    "                    delta = abs(E_temp - Ei)\n",
    "                    if delta > max_delta:\n",
    "                        max_delta = delta\n",
    "                        Ej = E_temp\n",
    "                        j = index\n",
    "            else:\n",
    "                j = i\n",
    "                while j == i:\n",
    "                    j = int(random.uniform(0, model.m))\n",
    "                Ej = calc_E(j, model)\n",
    "            return j, Ej\n",
    "        \n",
    "        yi = model.y[i]\n",
    "        Ei = calc_E(i, model)\n",
    "        model.E[i] = [1, Ei]\n",
    "        # 如果alpahi不满足KKT条件, 则进行之后的操作, 选择alphaj, 更新alphai与alphaj, 还有b\n",
    "        if (yi * Ei > model.toler and model.alphas[i] > 0) or (yi * Ei < -model.toler and model.alphas[i] < model.C):\n",
    "            # alphai不满足KKT条件\n",
    "            # 选择alphaj\n",
    "            j, Ej = select_j(Ei, i, model)\n",
    "            yj = model.y[j] \n",
    "            alpha1old = model.alphas[i].copy()\n",
    "            alpha2old = model.alphas[j].copy()\n",
    "            eta = model.mapped_data[i, i] + model.mapped_data[j, j] - 2 * model.mapped_data[i, j]   \n",
    "            if eta <= 0:\n",
    "                return 0\n",
    "            alpha2new_unclip = alpha2old + yj * (Ei - Ej) / eta\n",
    "            if yi == yj:\n",
    "                L = max(0, alpha2old + alpha1old - model.C)\n",
    "                H = min(model.C, alpha1old + alpha2old)\n",
    "            else:\n",
    "                L = max(0, alpha2old - alpha1old)\n",
    "                H = min(model.C, model.C - alpha1old + alpha2old)\n",
    "            if L == H:\n",
    "                return 0\n",
    "            alpha2new = clip_alpha(L, H, alpha2new_unclip)\n",
    "            if abs(alpha2new - alpha2old) < 0.00001:\n",
    "                return 0\n",
    "            \n",
    "            alpha1new = alpha1old + yi * yj * (alpha2old - alpha2new)\n",
    "            b1new = -Ei - yi * model.mapped_data[i, i] * (alpha1new - alpha1old) \\\n",
    "                    - yj * model.mapped_data[j, i] * (alpha2new - alpha2old) + model.b\n",
    "            b2new = -Ej - yi * model.mapped_data[i, j] * (alpha1new - alpha1old) \\\n",
    "                    - yj * model.mapped_data[j, j] * (alpha2new - alpha2old) + model.b\n",
    "            model.b = calc_b(b1new, b2new)\n",
    "            model.alphas[i] = alpha1new\n",
    "            model.alphas[j] = alpha2new\n",
    "            model.E[i] = [1, calc_E(i, model)]\n",
    "            model.E[j] = [1, calc_E(j, model)]\n",
    "            return 1\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T18:37:28.986634Z",
     "start_time": "2019-11-29T18:37:28.787623Z"
    },
    "code_folding": [],
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def accuracy(real, predict):\n",
    "    \"\"\"计算预测准确度\"\"\"\n",
    "    return np.sum(real == predict)/real.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 试验1：\n",
    "- 对于hinge_loss使用了比较rubost的adagrad以及early stopping，\n",
    "- 我们可以使用较大或较小的lamda=30或者0.3 都不用慌张,\n",
    "- learning rate 也设置成了比较大的1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T18:37:29.890686Z",
     "start_time": "2019-11-29T18:37:28.988634Z"
    },
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "此次adagrad的最小loss为第8200次：18.311536725256126，预测时使用了这时的w,b\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "(X_train, Y_train), (X_valid, Y_valid)=process_X()\n",
    "model = SVM()\n",
    "model.fit(X_train, Y_train,math_program=\"hinge_loss\", algo=\"hinge_loss_algo\", iternum=10000, lamda=30)\n",
    "Y_pred = model.predict(X_valid)\n",
    "print(f\"accuracy: {accuracy(Y_valid, Y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T18:37:29.900686Z",
     "start_time": "2019-11-29T18:37:29.893686Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.13610925],\n",
       "        [-0.17928064],\n",
       "        [ 0.57768   ],\n",
       "        [ 0.23562675]]), array([-1.96362268]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.w,model.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 试验2(对于对偶方程和非线性kernel函数):使用优化算法SQP直接求解alpha\n",
    "\n",
    "- 对于margin_C还是比较的robust：实验了一下C=0.05以上似乎都OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T18:37:30.250706Z",
     "start_time": "2019-11-29T18:37:29.902686Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....正在使用优化算法SQP直接求解alpha...\n",
      "------------------------------------\n",
      "迭代终止是否成功： True\n",
      "fmin： -2.59245614291428\n",
      "向量alpha: [0.01698859 0.         0.01456012 0.1        0.07795744 0.1\n",
      " 0.1        0.         0.         0.1        0.0879561  0.\n",
      " 0.         0.         0.1        0.         0.         0.09648196\n",
      " 0.         0.1        0.1        0.1        0.1        0.07479176\n",
      " 0.1        0.         0.03361904 0.1        0.1        0.1\n",
      " 0.         0.1        0.1        0.1        0.1        0.09797673\n",
      " 0.         0.         0.1        0.1        0.1        0.01595671\n",
      " 0.1        0.         0.1        0.         0.         0.\n",
      " 0.1        0.1        0.1        0.         0.1        0.01599255\n",
      " 0.1        0.1        0.         0.1        0.1        0.\n",
      " 0.1        0.07329402 0.1        0.1        0.1        0.1\n",
      " 0.1        0.0213345  0.1        0.03895072 0.1        0.\n",
      " 0.         0.         0.1        0.00844464 0.1        0.\n",
      " 0.         0.        ]\n",
      "等式约束1误差： [-1.80411242e-16]\n",
      "不等式约束1满足个数(alpha=0)： 26\n",
      "不等式约束2满足个数(alpha=C)： 40\n",
      "------------------------------------\n",
      "accuracy: 1.0000\n",
      "------------------------------------\n",
      "CPU times: user 324 ms, sys: 80.5 ms, total: 405 ms\n",
      "Wall time: 334 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "(X_train, Y_train), (X_valid, Y_valid)=process_X()\n",
    "model1 = SVM()\n",
    "model1.fit(X_train, Y_train,math_program=\"dual_model_nonliner_loss\", algo=\"dual_optm_algo\", margin_C=0.1)\n",
    "Y_pred = model1.predict(X_valid,isnolinear=True)\n",
    "print(\"------------------------------------\")\n",
    "print(f\"accuracy: {accuracy(Y_valid, Y_pred):.4f}\")\n",
    "print(\"------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 试验3(对偶非线性kernel函数):使用SMO求解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-29T18:37:30.705732Z",
     "start_time": "2019-11-29T18:37:30.252706Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迭代第0次：changed_alphas个数为 13\n",
      "迭代第1次：changed_alphas个数为 0\n",
      "退出迭代\n",
      "------------------------------------\n",
      "accuracy: 1.0000\n",
      "------------------------------------\n",
      "CPU times: user 644 ms, sys: 32.9 ms, total: 677 ms\n",
      "Wall time: 630 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "(X_train, Y_train), (X_valid, Y_valid)=process_X()\n",
    "model2 = SVM()\n",
    "model2.fit(X_train, Y_train,math_program=\"dual_model_smo\", algo=\"smo_algo\",margin_C=0.3, toler=0.03, iternum=3)\n",
    "Y_pred = model2.predict_smo(X_valid)\n",
    "print(\"------------------------------------\")\n",
    "print(f\"accuracy: {accuracy(Y_valid, Y_pred):.4f}\")\n",
    "print(\"------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-28T09:50:23.948549Z",
     "start_time": "2019-11-28T09:50:23.943549Z"
    }
   },
   "source": [
    "- 第一次尝试父类子类，没有把整个程序的代码设计的很好\n",
    "- 用了很多重复但不同的函数，导致整个结构不是很好，影响代码阅读，很抱歉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [
     "\n"
    ],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}